"""
Compara Dummy vs HF-E5 embeddings con ejemplos reales.
"""
from embeddings.base import DummyEmbedding, EmbeddingConfig
from embeddings.providers.hf_e5_embedding import HFMultilingualE5Embedding
import numpy as np

def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

def compare_embeddings():
    print("üî¨ COMPARACI√ìN: Dummy vs HF-E5 Embeddings\n")
    
    # Documentos
    docs = [
        "Python es un lenguaje de programaci√≥n usado en ciencia de datos y machine learning",
        "Java es un lenguaje orientado a objetos usado en aplicaciones empresariales",
        "HTML es un lenguaje de marcado para crear p√°ginas web",
        "JavaScript se utiliza para desarrollo web frontend y backend"
    ]
    
    # Query
    query = "¬øQu√© lenguaje usar para inteligencia artificial?"
    
    print(f"üìù Query: '{query}'\n")
    print("üìö Documentos:")
    for i, doc in enumerate(docs, 1):
        print(f"  {i}. {doc}")
    
    print("\n" + "="*80 + "\n")
    
    # ===== DUMMY EMBEDDINGS =====
    print("üé≤ DUMMY EMBEDDINGS (Aleatorio)\n")
    dummy = DummyEmbedding(EmbeddingConfig(dimension=384))
    
    query_vec_dummy = dummy.embed_text(query)
    doc_vecs_dummy = [dummy.embed_text(doc) for doc in docs]
    
    similarities_dummy = [
        cosine_similarity(query_vec_dummy, doc_vec) 
        for doc_vec in doc_vecs_dummy
    ]
    
    print("Similitudes:")
    for i, (doc, sim) in enumerate(zip(docs, similarities_dummy), 1):
        print(f"  Doc {i}: {sim:.4f} - {doc[:50]}...")
    
    best_dummy = np.argmax(similarities_dummy)
    print(f"\n‚úì Mejor match: Doc {best_dummy + 1} (score: {similarities_dummy[best_dummy]:.4f})")
    print(f"  '{docs[best_dummy][:60]}...'\n")
    
    print("="*80 + "\n")
    
    # ===== HF-E5 EMBEDDINGS =====
    print("üß† HF-E5 EMBEDDINGS (Sem√°ntico)\n")
    
    try:
        hf = HFMultilingualE5Embedding(
            EmbeddingConfig(
                model_name="intfloat/multilingual-e5-small",
                dimension=384
            )
        )
        
        query_vec_hf = hf.embed_query(query)
        doc_vecs_hf = [hf.embed_text(doc) for doc in docs]
        
        similarities_hf = [
            cosine_similarity(query_vec_hf, doc_vec) 
            for doc_vec in doc_vecs_hf
        ]
        
        print("Similitudes:")
        for i, (doc, sim) in enumerate(zip(docs, similarities_hf), 1):
            print(f"  Doc {i}: {sim:.4f} - {doc[:50]}...")
        
        best_hf = np.argmax(similarities_hf)
        print(f"\n‚úì Mejor match: Doc {best_hf + 1} (score: {similarities_hf[best_hf]:.4f})")
        print(f"  '{docs[best_hf][:60]}...'\n")
        
        # ===== COMPARACI√ìN =====
        print("="*80 + "\n")
        print("üìä AN√ÅLISIS:\n")
        
        if best_dummy == 0 and best_hf == 0:
            print("‚úÖ Ambos encontraron Python (correcto)")
            print(f"   Pero HF-E5 tiene mayor confianza: {similarities_hf[0]:.4f} vs {similarities_dummy[0]:.4f}")
        elif best_hf == 0 and best_dummy != 0:
            print("‚úÖ HF-E5 encontr√≥ Python (CORRECTO)")
            print(f"‚ùå Dummy encontr√≥ {docs[best_dummy][:40]}... (INCORRECTO)")
            print("\nüí° HF-E5 entiende que Python se usa para IA/ML")
            print("   Dummy solo hace matching aleatorio")
        else:
            print(f"HF-E5: Doc {best_hf + 1}")
            print(f"Dummy: Doc {best_dummy + 1}")
        
        print("\nüìà Mejora de precisi√≥n:")
        improvement = (similarities_hf[0] - similarities_dummy[0]) / similarities_dummy[0] * 100
        print(f"   HF-E5 es {abs(improvement):.1f}% {'m√°s' if improvement > 0 else 'menos'} preciso para el mejor resultado")
        
    except ImportError:
        print("‚ö†Ô∏è  HF-E5 no instalado. Instala con:")
        print("   pip install torch transformers sentence-transformers")

if __name__ == "__main__":
    compare_embeddings()