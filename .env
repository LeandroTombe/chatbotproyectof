# ========================================
# Ollama LLM Configuration
# ========================================
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
OLLAMA_TIMEOUT=180
OLLAMA_MAX_TOKENS=300
OLLAMA_TEMPERATURE=0.7

# Otros modelos disponibles:
#phi
#mistral
#codellama
#llama2
#tinyllama

# ========================================
# RAG Configuration
# ========================================
RAG_TOP_K=2
RAG_MIN_RELEVANCE=0.3
RAG_MAX_CONTEXT_LENGTH=1000
RAG_INCLUDE_SOURCES=True
RAG_STRICT_MODE=True
RAG_ENABLE_SECURITY=True

# ========================================
# Embeddings Configuration
# ========================================
#EMBEDDING_PROVIDER=dummy
#EMBEDDING_MODEL=dummy-embeddings
#EMBEDDING_DIMENSION=768


#otro embedding model disponible:
EMBEDDING_PROVIDER=hf-e5
EMBEDDING_MODEL=intfloat/multilingual-e5-small
EMBEDDING_DIMENSION=384

# ========================================
# Chunking Configuration  
# ========================================
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
CHUNK_SEPARATOR=\n\n

# ========================================
# Vector Store Configuration
# ========================================
VECTOR_STORE_TYPE=chroma
# Para usar ChromaDB: cambiar a VECTOR_STORE_TYPE=chroma
# Requiere: pip install chromadb

CHROMA_PERSIST_DIRECTORY=./data/chroma
CHROMA_COLLECTION_NAME=chatbot_collection
