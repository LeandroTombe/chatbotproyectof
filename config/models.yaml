# ============================================================================
# Ollama Model Profiles
# ============================================================================
# Predefined configurations for different Ollama models
# Use these as reference when selecting OLLAMA_MODEL in .env

phi:
  description: "Fast, low resource model - good for quick responses"
  timeout: 60
  max_tokens: 300
  temperature: 0.7
  recommended_for: "Quick testing, low-end hardware, fast responses"
  
llama3.2:
  description: "Balanced performance and quality"
  timeout: 120
  max_tokens: 500
  temperature: 0.7
  recommended_for: "General purpose, balanced systems"
  
mistral:
  description: "High quality responses, more resource intensive"
  timeout: 180
  max_tokens: 1000
  temperature: 0.7
  recommended_for: "Production use, high-quality responses"
  
codellama:
  description: "Specialized for code and technical content"
  timeout: 150
  max_tokens: 800
  temperature: 0.5
  recommended_for: "Technical documentation, code examples"
  
llama2:
  description: "Legacy model, stable and well-tested"
  timeout: 120
  max_tokens: 400
  temperature: 0.7
  recommended_for: "Compatibility, proven reliability"

# ============================================================================
# Usage Instructions
# ============================================================================
# 1. Choose a model from the list above
# 2. Set OLLAMA_MODEL in your .env file to the model name (e.g., phi)
# 3. Optionally adjust timeout, max_tokens, and temperature in .env
# 4. Make sure the model is pulled: ollama pull <model_name>
