# ğŸ¤– GuÃ­a de Uso - Chat RAG con Ollama

## ğŸš€ Inicio RÃ¡pido

### 1. Preparar tus PDFs

Coloca tus archivos PDF en la carpeta `data/pdfs/`:

```bash
data/
  pdfs/
    documento1.pdf
    documento2.pdf
    documento3.pdf
```

### 2. Verificar que Ollama estÃ© corriendo

```bash
# Ver modelos disponibles
ollama list

# Si Ollama no estÃ¡ corriendo, inÃ­cialo:
ollama serve
```

### 3. Ejecutar el sistema

```bash
python main.py
```

## ğŸ“‹ Proceso del Demo

El script `main.py` te guiarÃ¡ a travÃ©s de estos pasos:

### **Demo 1: Procesar un PDF individual**
- Carga un archivo PDF
- Lo divide en chunks
- Genera embeddings
- Lo almacena en el vector store

### **Demo 2: Procesamiento en lote**
- Procesa todos los PDFs de la carpeta
- Muestra estadÃ­sticas de Ã©xito/fallo

### **Demo 3: BÃºsqueda semÃ¡ntica**
- Busca informaciÃ³n en los documentos procesados
- Muestra los chunks mÃ¡s relevantes

### **Demo 4: ExtracciÃ³n de contexto**
- Obtiene contexto formateado para RAG

### **Demo 5: Chat Setup**
- Inicializa el servicio de chat con Ollama
- Verifica disponibilidad del modelo

### **Demo 6: Chat RAG**
- Pregunta de ejemplo con contexto de documentos
- Muestra fuentes utilizadas

### **Demo 7: Chat Interactivo** ğŸ’¬
- **AquÃ­ puedes hacer tus preguntas!**
- El sistema busca en tus PDFs
- El LLM responde usando el contexto
- Cita las fuentes utilizadas

## ğŸ’¬ Uso del Chat Interactivo

### Comandos disponibles:

```
ğŸ‘¤ You: tu pregunta aquÃ­          â†’ Hacer una pregunta
ğŸ‘¤ You: clear                      â†’ Limpiar historial de conversaciÃ³n
ğŸ‘¤ You: history                    â†’ Ver historial de mensajes
ğŸ‘¤ You: quit / exit / q            â†’ Salir
```

### Ejemplos de preguntas:

```
ğŸ‘¤ You: Â¿CuÃ¡l es el tema principal del documento?
ğŸ‘¤ You: Resume los puntos clave
ğŸ‘¤ You: Â¿QuÃ© dice sobre [tema especÃ­fico]?
ğŸ‘¤ You: Dame mÃ¡s detalles sobre [concepto]
```

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Cambiar el modelo de Ollama

Edita `main.py` en la funciÃ³n `setup_chat_service()`:

```python
rag_service = setup_chat_service(
    retriever=retriever,
    model_name="llama3.2",  # â† Cambia aquÃ­ el modelo
    base_url="http://localhost:11434"
)
```

### Modelos recomendados:

- **llama3.2** (2GB) - RÃ¡pido, buena calidad
- **llama2** (3.8GB) - Muy popular, balanceado
- **mistral** (4.1GB) - Excelente rendimiento
- **codellama** - Especializado en cÃ³digo

Para descargar un modelo:
```bash
ollama pull llama3.2
ollama pull mistral
```

### Ajustar parÃ¡metros RAG

En `main.py`, modifica `RAGConfig`:

```python
rag_config = RAGConfig(
    top_k=3,                    # NÃºmero de documentos a recuperar
    min_relevance=0.3,          # Score mÃ­nimo de relevancia (0-1)
    max_context_length=2000,    # MÃ¡ximo de caracteres de contexto
    include_sources=True,       # Mostrar fuentes en respuesta
    system_prompt="..."         # Prompt del sistema
)
```

### Ajustar parÃ¡metros del LLM

En `main.py`, modifica `LLMConfig`:

```python
llm_config = LLMConfig(
    model_name="llama3.2",
    temperature=0.7,       # Creatividad (0=conservador, 1=creativo)
    max_tokens=512,        # Longitud mÃ¡xima de respuesta
    timeout=60            # Timeout en segundos
)
```

## ğŸ› ï¸ SoluciÃ³n de Problemas

### âŒ "Ollama is not running"

```bash
# Iniciar Ollama
ollama serve

# En otra terminal, verificar
ollama list
```

### âŒ "Model 'llama2' is not available"

```bash
# Descargar el modelo
ollama pull llama2

# O usar el que ya tienes
ollama pull llama3.2
```

### âŒ "No results found"

- AsegÃºrate de que los PDFs se hayan procesado correctamente
- Verifica que haya archivos en `data/pdfs/`
- Revisa que los documentos contengan texto (no solo imÃ¡genes)

### âŒ Error de conexiÃ³n timeout

**SÃ­ntoma**: `Request to Ollama timed out after XXs`

**Causas**:
- Primera consulta (modelo cargÃ¡ndose en memoria)
- Contexto muy largo enviado al LLM
- Procesador lento o recursos limitados

**Soluciones**:

1. **Espera un poco mÃ¡s en la primera consulta** - El sistema ahora precalienta el modelo automÃ¡ticamente

2. **Si sigue ocurriendo**, los parÃ¡metros ya estÃ¡n optimizados en `main.py`:
   - Timeout: 120 segundos (fue aumentado)
   - max_tokens: 300 (reducido para respuestas mÃ¡s rÃ¡pidas)
   - max_context_length: 1000 (reducido para menos texto)
   - top_k: 2 (menos documentos de contexto)

3. **Para ajustes adicionales**, edita en `main.py`:
   ```python
   llm_config = LLMConfig(
       timeout=180  # Aumenta aÃºn mÃ¡s si es necesario
   )
   
   rag_config = RAGConfig(
       top_k=1,              # Usa solo 1 documento
       max_context_length=500  # Reduce mÃ¡s el contexto
   )
   ```

4. **Verifica Ollama**:
   ```bash
   # Ver si el modelo estÃ¡ cargado
   ollama ps
   
   # Si ves 100% CPU, estÃ¡ trabajando
   ```

5. **Usa un modelo mÃ¡s rÃ¡pido**:
   ```bash
   # Descargar un modelo mÃ¡s pequeÃ±o
   ollama pull phi
   
   # Luego en main.py cambia a model_name="phi"
   ```

### âŒ Error de conexiÃ³n timeout

- Aumenta el timeout en `LLMConfig`
- Usa un modelo mÃ¡s pequeÃ±o (llama3.2 en lugar de llama2)

## ğŸ“Š Flujo Completo

```
1. PDFs en data/pdfs/
        â†“
2. Procesamiento (main.py Demo 1-2)
   - ExtracciÃ³n de texto
   - DivisiÃ³n en chunks
   - GeneraciÃ³n de embeddings
   - Almacenamiento en vector store
        â†“
3. Chat Interactivo (Demo 7)
   - Tu pregunta
        â†“
   - BÃºsqueda semÃ¡ntica en vector store
        â†“
   - RecuperaciÃ³n de chunks relevantes
        â†“
   - ConstrucciÃ³n de contexto
        â†“
   - GeneraciÃ³n de respuesta con Ollama
        â†“
   - Respuesta + Fuentes citadas
```

## ğŸ¯ Ejemplo Completo

```bash
# 1. Colocar PDFs
cp mis_documentos/*.pdf data/pdfs/

# 2. Ejecutar el sistema
python main.py

# 3. Seguir los demos (presionar ENTER)
# ...

# 4. En el chat interactivo:
ğŸ‘¤ You: Â¿De quÃ© trata este documento?

ğŸ¤” Thinking...

ğŸ¤– Assistant: Este documento trata principalmente sobre [respuesta basada en el contenido]...

ğŸ“š [2 sources used]

ğŸ‘¤ You: Dame mÃ¡s detalles sobre [tema]

ğŸ¤– Assistant: [Respuesta detallada]...

ğŸ‘¤ You: quit
```

## ğŸ“ Notas Importantes

- **Embeddings**: Por defecto usa DummyEmbedding (para demo). Para producciÃ³n, considera usar HuggingFace E5.
- **Memoria**: El chat mantiene historial de conversaciÃ³n (configurable)
- **Privacidad**: Todo se ejecuta localmente, no se envÃ­an datos a APIs externas
- **Rendimiento**: Primera pregunta puede tardar mÃ¡s (carga del modelo)

## ğŸ”— Recursos Adicionales

- [Ollama Documentation](https://ollama.ai)
- [Available Models](https://ollama.ai/library)
- README_DEMO.md - DocumentaciÃ³n tÃ©cnica completa
- QUICKSTART.md - GuÃ­a rÃ¡pida de instalaciÃ³n

---

Â¡Disfruta conversando con tus documentos! ğŸš€
